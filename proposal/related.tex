\section{Related Work}

\paragraph{Corpus-wide Naming Rules}  Abebe et al.\/ investigate identifier quality  based on a catalog of Lexicon Bad Smells~\cite{abebe:wcre12}. 
Caprile and Tonella leverage pre-defined syntax and identifier dictionary to replace terms with their synonyms to ensure naming consistency~\cite{caprile:reconstruct}. Dei{\ss}enb{\"{o}}ck and Pizka proposed a formal model for concise and consistent naming, which requires a mapping from a concept domain to a lexical domain~\cite{ccnames:Pizka06}. Lawrie et al.\/ use hard-coded naming rules such as \textit{an identifier should not include another identifier in its name}~\cite{syntacticcc:SCAM06}.  Arnaoudova et al.\/ define a family of linguistic anti-patterns such as \textit{set* methods should not return an object} and investigate misunderstanding caused by anti-patterns~\cite{antipattern:CSMR13}. None of these approaches detect project-specific naming inconsistency.  
Allamanis et al. \/ build a tool called Naturalize to learn and enforce coding conventions~\cite{birdfse14:convension}. It uses n-gram based statistical natural language processing model to suggest natural identifier names and formatting conventions at the level of type names, method calls, and variable names. 
%  The only overlapping target in scope is the variable name recommendations reported by Naturalize. We run Naturalize on the same 39 projects using its default settings and compare variable name reports. We use a few examples to illustrate the difference between \niche and Naturalize. Naturalize misses inconsistent object names such as `\codefont{repStore}', which violates the commonly used name pattern (`\codefont{*FS}') found by type-use pattern analysis in \niche. On the other hand, Naturalizes suggests renaming `\codefont{local\_folder}' to `\codefont{localFolder}'. \niche does not produce such naming suggestion, as it focuses on checking name consistency based on the code's semantic functionality.
H{\o}st et al. \/ build a tool called Lancelot to find poor method names by comparing syntactic code features, such as `\textit{contains a loop}'~\cite{nameBug:ECOOP09} .  
%The only overlapping scope for Lancelot and \niche is the inconsistent method names found by \niche and the `\textit{method naming bugs}' from Lancelot. Lancelot misses inconsistent method names such as `\codefont{setSyncFlag}' which does not change the field value because it checks syntactic features only without considering the purity of a method. Lancelot reports  `\codefont{setExit}' method as a naming bug, since it returns the object itself (\codefont{return this}), but in this case, \niche classifies it as consistent because the method is impure. 
Buse and Weimer develop a readability metric for Java by training data from human annotators~\cite{buse:readability}. They find that their readability metric exhibits a significant level of correlation with static analysis warnings found by FindBug~\cite{FindBug:OOPSLA06}. Though their work connects the readability of identifier names to  static analysis warnings, they do not study the correlation between inconsistent names and bug fixes. 

\paragraph{Defect Study}  Abebe et al.\/ find correlation between naming rule violations and  coupling metrics~\cite{abebe:wcre12,ckmetrics:oopsla91}. Butler et al.\/ find correlation between FindBug's static analysis warnings and violations of hard-coded rules~\cite{FindBug:OOPSLA06,butler:csmr10}. Since static analysis tools suffer from 30\% to 100\% false positive rates~\cite{kremenekWarn:fse04}, we cannot conclude whether naming inconsistency is correlated with actual defects. Boogerd et al.\/ find that only 10 out of 88 hard-coded naming rules have positive correlation with defect density~\cite{boogerd:icsm08}. We differ from prior studies by mining rules from a corpus and studying how these rule violations relate to actual defects. Our study also finds that the confusion and misunderstanding caused by inconsistent name may propagate to their callers and that the lifetime of inconsistent names is shorter than the rest. 

%\paragraph{Mining Usage Patterns} 
 
\paragraph{Feature Location} Poshyvanyk et al. \/ use information retrieval approach to locate a queried feature in source code~\cite{Denys:FCA12}. They  evaluate the similarity between documents and user query and cluster the source code based on formal concept analysis.  Portforlio~\cite{Portfolio:DenysICSE11} and Export~\cite{Export:DenysASE13} identify related functions by combining both latent structure similarity and lexical information similarity. Our feature location approach is similar to~\cite{Portfolio:DenysICSE11} yet we focus on suggesting structured implementation for the feature rather than identify feature location.
Rastkar et al. \/ summarize the structure of multiple instances of a crosscutting concern in natural language, yet they only extract structural facts in the level of method signature and class hierarchy~\cite{Murphy:nlConcern11}. We make it one step further to suggest structured feature implementation based on  user query.

\paragraph{Code Search} Our example clustering approach is similar to some prior works that extract representative examples for specific APIs or user query. 
MAPO~\cite{MAPO:ECOOP09} leverages frequent call sequences to cluster the usage of specific APIs and rank abstract usage patterns based on the context similarity. Buse et al\/\cite{Buse:apiICSE12} propose to generate abstract API usages by synthesizing code examples using symbolic execution for a particular API. Different from these works that generate abstract usage patterns for specific API or data type, SNIFF~\cite{sniff:Sen09} performs type-based intersection of code chunks based on the keywords in the free-form query and cluster the common part of the code chunks for concrete code examples. However, these works   only focus on providing code examples based on the popularity or textural relevance while developers have to manually resolve structural dependencies before reusing the examples.  MUSE~\cite{MUSE:MarcusICSE15} addresses this limitation using slicing to generate concrete usage examples and selects the most representative ones based on the popularity and readability while  Keivanloo et al\/\cite{spotWork:ICSE14} uses clone detection to cluster examples involving loops and conditions.  There exists a number of code example suggestion tools that recommend call chains~\cite{Mandelin:jungloid05, parseWeb:ASE07, Xsnippet:OOPSLA06}  or contexts~\cite{Holmes:structural05, Prompter:MSR14}. But none of  these tools presents code example in a structured manner and identify  common features for each code example cluster, which helps developers to reuse the code examples. 

%only recommend code examples in the method level which make them insufficient for  reuse tasks across multiple classes.   We make it one step further to  identify  both common features and alternative features.

\paragraph{Code Reuse}  Although some approaches advocate refactoring code rather than reuse code~\cite{fowler:refactoring}, recent researches have found that these kind of `clone' cannot be easily refactored~\cite{Kim:cloneGenealogy05} and have to be modified to meeting requirements in new context~\cite{Selby:largeReuse05}.  Jigsaw~\cite{Cottrell:jigsaw08} supports small-scale integration of source code into target system  between the example and target context. Based on its ancestor~\cite{Cottrell:generalize07} that identifies structural correspondence based on AST similarity, it greedily matches each element between two contexts, transforms correspondent elements to the target context, and simply copies the source element to the target if it does not correspond with any element in the target. Unfortunately, developer has to provide source and target to enable a one-to-one transformation and resolve all dependencies when pasting code to the target.  Our approach overcomes these two limitations: we extract common functionalities from multiple examples and  identifies how related elements interact with main features.  Other works on code reuse include
Gilligan~\cite{Holmes:reuse07} and Procrustes~\cite{Holmes:ASE09} which try to address the problem of source code integration in the context of medium or large-scale reuse tasks. They automatically suggest program elements that are easy to reuse based on structural relevance and cost of reuse in the source context, and guide users to investigate and plan a non-trivial reuse task. They assume that developers have a perfect example at hand, and they can finish the reuse task by resolving all dependency conflicts and integrating the example to the desired context.  We don't have this assumption and our tool helps users identify the best-fit example. Our idea of leveraging multiple examples to discover commonality is similar to LASE~\cite{LASE:ICSE13}, which applies similar but not identical changes to multiple code locations based on context similarity.   Our approach works in a similar manner of Programing-by-Example in the context of code reuse task. LASE requires users to specify multiple input examples and search for the third one, while our approach uses free-form queries to obtain hundreds of examples and cluster them based on their common features.

%focuses on a task-based code reuse across different methods or even different classes, while LASE is confined to the systematic edit within a single method and requires users to specify all input examples. 


%However, we note that it is not easy to identify a good example as an example is always interleaving with other auxiliary features that should not be integrated. We observe that it is equally difficult, if not more so, to distinguish the major functionality and auxiliary ones from multiple examples than to identify related elements in a pragmatic reuse plan. We target the problem to identify the major features across different reusable examples and leverage Procrustes to evaluate the cost of reuse when recommending the best-fit reusable plan.
