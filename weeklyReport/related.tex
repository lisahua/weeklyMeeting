
\section{Related Work}
%\input{table}
%\noindent{\textbf{Extract partial program from informal documentation.}}  Lightweight regular expressions and information retrieval techniques have been widely used to resolve the links between source code elements and documentations. Bacchelli et al.~\cite{Bacchelli:emailICSE10} use regular expressions to  identify code elements in email discussions based on programming naming conventions. Antoniol et al.~\cite{Antoniol:Link02} apply a probabilistic Vector Space Model to resolve terms while Marcus et al.~\cite{Marcus:LinkICSE03} use Latent Semantic Indexing to recover links. However, both regular expression and information retrieval techniques suffer from a low precision and recall around 0.5. RecoDoc~\cite{RecoDoc:ICSE12} leverages term context to extract code-like terms and use partial program analysis~\cite{partialProgram:OOPSLA08} and a set of structural/lexical heuristic filters to resolve ambiguous terms. ACE~\cite{PeterACE:ICSE13} extends RecoDoc with island parser to identify code-like terms and reparses each document to resolve ambiguous terms using term's context. We leverage ACE to extract partial program from informal documentations and use partial program analysis~\cite{partialProgram:OOPSLA08}  to infer type declaration and resolve method binding ambiguity. We advance these tools in the sense that we not only identify partial program as code example, but also help cluster similar examples and integrate reusable examples to the target context. 
\noindent{\textbf{Feature Location Tools.}}  Poshyvanyk et al.~\cite{Denys:FCA12} use information retrieval approach to locate a queried feature in source code. They  evaluate the similarity between documents and user query and cluster the source code based on formal concept analysis.  Portforlio~\cite{Portfolio:DenysICSE11} and Export~\cite{Export:DenysASE13} identify related functions by combining both latent structure similarity and lexical information similarity. Our feature location approach is similar to~\cite{Portfolio:DenysICSE11} yet we focus on suggesting implementation for the feature rather than identify feature location.
Rastkar et al.~\cite{Murphy:nlConcern11} summarize the structure of multiple instances of a crosscutting concern in natural language, yet they only extract structural facts in the level of method signature and class hierarchy. We make it one step further to suggest feature implementation based on the context and user query.

%Hill et al.~\cite{Hill:FindConcept07} leverage natural language analysis to 


\noindent{\textbf{Code Search Tools.}} 
Our example clustering approach is similar to some prior works that extract representative examples for specific APIs or user query. 
MAPO~\cite{MAPO:ECOOP09} leverages frequent call sequences to cluster the usage of specific APIs and rank abstract usage patterns based on the context similarity. Buse et al\/\cite{Buse:apiICSE12} propose to generate abstract API usages by synthesizing code examples using symbolic execution for a particular API. Different from these works that generate abstract usage patterns for specific API or data type, SNIFF~\cite{sniff:Sen09} performs type-based intersection of code chunks based on the keywords in the free-form query and cluster the common part of the code chunks for concrete code examples. However, these works   only focus on providing code examples based on the popularity or textural relevance while developers have to manually resolve structural dependencies before reusing the examples.  MUSE~\cite{MUSE:MarcusICSE15} addresses this limitation using slicing to generate concrete usage examples and selects the most representative ones based on the popularity and readability while  Keivanloo et al\/\cite{spotWork:ICSE14} uses clone detection to cluster examples involving loops and conditions. We make it one step further to  identify structural correspondence to identify both common features and alternative features.  There exists a number of code example suggestion tools that recommend call chains~\cite{Mandelin:jungloid05, parseWeb:ASE07, Xsnippet:OOPSLA06}  or contexts~\cite{Holmes:structural05, Prompter:MSR14}. But these tools can only recommend code examples in the method level which make them insufficient for  reuse tasks across multiple classes.  


\noindent{\textbf{Identify structural correspondence for code reuse}}   Although some approaches advocate refactoring code rather than reuse code~\cite{fowler:refactoring}, recent researches have found that these kind of `clone' cannot be easily refactored~\cite{Kim:cloneGenealogy05} and have to be modified to meeting requirements in new context~\cite{Selby:largeReuse05}.  Jigsaw~\cite{Cottrell:jigsaw08} supports small-scale integration of source code into target system  between the example and target context. Based on its ancestor~\cite{Cottrell:generalize07} that identifies structural correspondence based on AST similarity, it greedily matches each element between two contexts, transforms correspondent elements to the target context, and simply copies the source element to the target if it does not correspond with any element in the target. Unfortunately, developer has to provide source and target to enable a one-to-one transformation and resolve all dependencies when pasting code to the target.  Our approach overcomes these two limitations: we extract common functionalities from multiple examples and  identifies how related elements interact with main features in a common way to resolve dependencies based on the mapping from the source to the target.  Our idea of leveraging multiple examples to discover commonality and eliminate specificity is similar to LASE~\cite{LASE:ICSE13}, which applies similar but not identical changes to multiple code locations based on context similarity. Our approach works in a similar manner of Programing-by-Example, but focuses on a task-based code reuse across different methods or even different classes, while LASE is confined to the systematic edit within a single method and requires users to specify all input examples. Other related works on code reuse include
Gilligan~\cite{Holmes:reuse07} and Procrustes~\cite{Holmes:ASE09} which try to address the problem of source code integration in the context of medium or large-scale reuse tasks. They automatically suggest program elements that are easy to reuse based on structural relevance and cost of reuse in the source context, and guide users to investigate and plan a non-trivial reuse task. They assume that developers have a perfect example at hand, and they can finish the reuse task by resolving all dependency conflicts and integrating the example to the desired context. However, we note that it is not easy to identify a good example as an example is always interleaving with other auxiliary features that should not be integrated. We observe that it is equally difficult, if not more so, to distinguish the major functionality and auxiliary ones from multiple examples than to identify related elements in a pragmatic reuse plan. We target the problem to identify the major features across different reusable examples and leverage Procrustes to evaluate the cost of reuse when recommending the best-fit reusable plan.






